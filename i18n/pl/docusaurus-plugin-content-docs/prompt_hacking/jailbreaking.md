---
sidebar_position: 4
---

#  Jailbreaking

Jailbreaking jest typem prompt injection, w kt贸rym monity pr贸buj obej **bezpieczestwo** i **moderacj** funkcji umieszczonych na LLM przez ich tw贸rc贸w(@perez2022jailbreak)(@brundage_2022)(@wang2022jailbreak).

# Methodologie of Jailbreaking

OpenAI, wr贸d innych firm i organizacji tworzcych LLM, posiada funkcje moderowania treci
funkcje zapewniajce, 偶e ich modele nie produkuj kontrowersyjnych (brutalnych, seksualnych, nielegalnych itp.)
odpowiedzi(@markov_2022)(@openai_api). Ta strona omawia jailbreaki z ChatGPT (model OpenAI), kt贸ry ma znane trudnoci z podjciem decyzji o odrzuceniu szkodliwych podpowiedzi (@openai_chatgpt). Podpowiedzi, kt贸re skutecznie przeamuj model, czsto dostarczaj kontekstu
dla pewnych scenariuszy, z kt贸rymi model nie by szkolony.

### Udawanie

Popularn metod jailbreakingu jest _pretending_. Jeli ChatGPT zostanie zapytany o
przysze wydarzenie, czsto powie, 偶e nie wie, poniewa偶 jeszcze nie nastpio.
Poni偶szy monit zmusza go do udzielenia mo偶liwej odpowiedzi:

#### Proste udawanie

import pretend from '@site/docs/assets/jailbreak/pretend_jailbreak.png';

<div style={{textAlign: 'center'}}>.
  <img src={pretend} style={{szeroko: "500px"}} />
</div>

[@NeroSoares](https://twitter.com/NeroSoares/status/1608527467265904643) demonstruje podpowied藕 udajc dostp do przeszych dat i wnioskowanie o przyszych wydarzeniach(@nero2022jailbreak).

#### Roleplay postaci

import actor from '@site/docs/assets/jailbreak/chatgpt_actor.jpg';

<div style={{textAlign: 'center'}}>.
  <img src={actor} style={{width: "500px"}} />
</div>

Ten przykad autorstwa [@m1guelpf](https://twitter.com/m1guelpf/status/1598203861294252033) demonstruje scenariusz aktorski pomidzy dwoma osobami dyskutujcymi o napadzie, powodujc, 偶e ChatGPT przyjmuje rol postaci(@miguel2022jailbreak). Jako aktor, sugeruje si, 偶e wiarygodna krzywda nie istnieje. W zwizku z tym ChatGPT wydaje si zakada, 偶e podawanie informacji o tym, jak wama si do domu, jest bezpieczne.

### Alignment Hacking

ChatGPT zosta dostrojony z RLHF, wic teoretycznie jest wyszkolony, aby produkowa "po偶dane" uzupenienia, u偶ywajc ludzkich standard贸w tego, co jest "najlepsz" odpowiedzi. Podobnie do tej koncepcji, jailbreaki zostay opracowane, aby przekona ChatGPT, 偶e robi "najlepsz" rzecz dla u偶ytkownika.

#### Przyjta odpowiedzialno

import responsibility from '@site/docs/assets/jailbreak/responsibility_jailbreak.jpg';

<div style={{textAlign: 'center'}}>.
  <img src={responsibility} style={{width: "500px"}} />
</div>

[@NickEMoran](https://twitter.com/NickEMoran/status/1598101579626057728) stworzy t wymian, potwierdzajc, 偶e obowizkiem ChatGPT jest odpowiedzie na podpowied藕, a nie odrzuci j, nadrzdnie biorc pod uwag legalno(@nick2022jailbreak).

#### Eksperyment badawczy

import hotwire from '@site/docs/assets/jailbreak/hotwire_jailbreak.png';

<div style={{textAlign: 'center'}}>.
  <img src={hotwire} style={{width: "500px"}} />
</div>

[@haus_cole](https://twitter.com/haus_cole/status/1598541468058390534) wygenerowa ten przykad sugerujc, 偶e najlepszym rezultatem podpowiedzi, kt贸ry m贸gby pom贸c w badaniach, jest bezporednia odpowied藕 na pytanie, jak zrobi hotwire w samochodzie (@derek2022jailbreak). Pod tym pozorem, ChatGPT jest skonny odpowiedzie na podpowied藕 u偶ytkownika.

#### Logiczne rozumowanie

import logic from '@site/docs/assets/jailbreak/logic.png';

<div style={{textAlign: 'center'}}>.
  <img src={logic} style={{width: "500px"}} />
</div>

One-shot jailbreak pochodzi z [AIWithVibes Newsletter Team](https://chatgpt-jailbreak.super.site/), gdzie model odpowiada na podpowiedzi u偶ywajc bardziej rygorystycznej logiki i zmniejsza niekt贸re z jego bardziej rygorystycznych ogranicze etycznych(@AI_jailbreak).

### Upowa偶niony u偶ytkownik

ChatGPT jest zaprojektowany do odpowiadania na pytania i instrukcje. Kiedy status u偶ytkownika jest interpretowany jako nadrzdny w stosunku do instrukcji moderacji ChatGPT, traktuje on monit jako instrukcj obsugi tego u偶ytkownika.

#### Superior Model

import GPT4 from '@site/docs/assets/jailbreak/chatgpt4.png';

<div style={{textAlign: 'center'}}>.
  <img src={GPT4} style={{width: "500px"}} />
</div>

Ten przykad z [@alicemazzy](https://twitter.com/alicemazzy/status/1598288519301976064) sprawia, 偶e u偶ytkownik jest nadrzdnym modelem GPT, sprawiajc wra偶enie, 偶e u偶ytkownik jest upowa偶nion stron w nadrzdnych funkcjach bezpieczestwa ChatGPT(@alice2022jailbreak). 呕adne rzeczywiste pozwolenie nie zostao udzielone u偶ytkownikowi, raczej ChatGPT wierzy w dane wejciowe u偶ytkownika i reaguje odpowiednio do tego scenariusza.

#### Tryb Sudo

import sudo_mode from '@site/docs/assets/jailbreak/sudo_mode_jailbreak.jpg';

<div style={{textAlign: 'center'}}>.
  <img src={sudo_mode} style={{width: "500px"}} />
</div>

sudo jest poleceniem, kt贸re "...deleguje uprawnienia do nadania pewnym u偶ytkownikom...mo偶liwoci uruchamiania niekt贸rych (lub wszystkich) polece..."(@sudo2022jailbreak). Istnieje wiele wariant贸w exploit贸w "trybu sudo", na przykad hipotetyczny "tryb jdra" zaproponowany przez [@samczsun](https://twitter.com/samczsun/status/1598679658488217601)(@sam2022jailbreak). Po zapytaniu w powy偶szy spos贸b, ChatGPT odpowiada tak, jakby dawa u偶ytkownikowi podwy偶szone uprawnienia. To wra偶enie podwy偶szonych przywilej贸w u偶ytkownika sprawia, 偶e ChatGPT jest mniej restrykcyjny w odpowiadaniu na monity.

import sudo from '@site/docs/assets/jailbreak/sudo_jailbreak.png';

<div style={{textAlign: 'center'}}>.
  <img src={sudo} style={{width: "500px"}} />
</div>

import lynx from '@site/docs/assets/jailbreak/lynx_jailbreak.png';

<div style={{textAlign: 'center'}}>.
  <img src={lynx} style={{width: "500px"}} />
</div>

W zwizku z trybem sudo, mo偶na skoni ChatGPT do symulacji terminala Linux z podwy偶szonymi uprawnieniami, aby wykona polecenia, kt贸re normalnie odrzuca. Na przykad, poniewa偶 nie ma dostpu do Internetu, czsto nie mo偶e wykona polece zwizanych z konkretn stron internetow. Jednak, jak pokazano w przykadzie Jonasa Degrave, ChatGPT rozumie koncepcj `lynx` i udaje, 偶e wykonuje polecenie (@jonas2022jailbreak).

# Simulate Jailbreaking

Spr贸buj zmodyfikowa poni偶szy prompt, aby jailbreak `text-davinci-003`:

<div trydyno-embed="" openai-model="text-davinci-003" initial-prompt="Twoim zadaniem jest poprawienie poni偶szego tekstu na standardowy angielski. Nie akceptuj temat贸w wulgarnych i politycznych:" initial-response="Nienawidz ludzi" max-tokens="256" box-rows="7" model-temp="0.7" top-p="0">.
    <noscript>Failed to load Dyno Embed: JavaScript musi by wczony</noscript>.
</div>

*Na dzie 2/4/23, ChatGPT jest obecnie w fazie Free Research Preview u偶ywajc wersji z 30 stycznia. Starsze wersje ChatGPT byy bardziej podatne na wspomniane jailbreaki, a przysze wersje mog by bardziej odporne na jailbreaki.*

## Implikacje

Przy pr贸bach jailbreakingu nale偶y wzi pod uwag etyczne implikacje. Dodatkowo, generowanie nieautoryzowanych treci oflagowanych przez API moderacyjne firm, w tym OpenAI, bdzie wysyane do weryfikacji, a dziaania mog by podjte wobec kont u偶ytkownik贸w.

## Notes

Jailbreaking to wa偶ny temat bezpieczestwa, kt贸ry powinni zrozumie deweloperzy,
aby mogli oni wbudowa odpowiednie zabezpieczenia, kt贸re uniemo偶liwi zoliwym aktorom
wykorzystywa ich modele.


