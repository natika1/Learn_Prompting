---
sidebar_position: 1000
---

# 📚 Bibliografia

Strona zawiera zorganizowaną listę wszystkich papierów używanych przez ten kurs.
Referaty są uporządkowane według tematów.

**Aby zacytować ten kurs, użyj dostarczonego cytatu w repozytorium Github.

🔵 = Referat bezpośrednio cytowany w tym kursie. Inne referaty poinformowały mnie o zrozumieniu tematu.

Uwaga: ponieważ [ani GPT-3 ani GPT-3 Instruct paper nie odpowiadają modelom davinci](https://twitter.com/janleike/status/1584618242756132864), staram się nie
cytować ich jako takich.

# Prompt Engineering Strategies

#### Łańcuch myśli(@wei2022chain) 🔵

#### Zero Shot Chain of Thought(@kojima2022large) 🔵

#### Self Consistency(@wang2022selfconsistency) 🔵

#### What Makes Good In-Context Examples for GPT-3?(@liu2021makes) 🔵

### Ask-Me-Anything Prompting(@arora2022ama) 🔵

#### Generated Knowledge(@liu2021generated) 🔵

#### Recitation-Augmented Language Models(@sun2022recitationaugmented) 🔵

#### Rethinking the role of demonstrations(@min2022rethinking) 🔵

#### Scratchpads(@nye2021work)

#### Maieutic Prompting(@jung2022maieutic)

#### STaR(@zelikman2022star)

#### Least to Most(@zhou2022leasttomost) 🔵


## Niezawodność

#### MathPrompter(@imani2023mathprompter) 🔵

#### The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning(@ye2022unreliability) 🔵

#### Prompting GPT-3 do rzetelności(@si2022prompting)

#### Diverse Prompts(@li2022advance) 🔵

#### Calibrate Before Use: Improving Few-Shot Performance of Language Models(@zhao2021calibrate) 🔵

#### Wzmocniona samoświadomość(@mitchell2022enhancing)

#### Bias and Toxicity in Zero-Shot CoT(@shaikh2022second) 🔵

#### Konstytucyjna AI: Bezszkodowość od AI Feedback (@bai2022constitutional) 🔵

#### Compositional Generalization - SCAN(@lake2018scan)
## Automated Prompt Engineering

#### AutoPrompt(@shin2020autoprompt) 🔵

#### Automatic Prompt Engineer(@zhou2022large)

## Modele

### Modele językowe

#### GPT-3(@brown2020language) 🔵

#### GPT-3 Instruktaż(@ouyang2022training) 🔵

#### PaLM(@chowdhery2022palm) 🔵

#### BLOOM(@scao2022bloom) 🔵

#### BLOOM+1 (więcej języków/ 0 ulepszeń strzałów)(@yong2022bloom1)

#### Jurassic 1(@lieberjurassic) 🔵

#### GPT-J-6B(@wange2021gptj)

#### Roberta(@liu2019roberta)

### Image Models

#### Stable Diffusion(@rombach2021highresolution) 🔵

#### DALLE(@ramesh2022hierarchical) 🔵

# Soft Prompting

#### Soft Prompting(@lester2021power) 🔵.

#### Interpretowalne Dyskretne Miękkie Prompts(@khashabi2021prompt) 🔵.

## Datasets

#### MultiArith(@roy-roth-2015-rozwiązanie) 🔵.

#### GSM8K(@cobbe2021training) 🔵

#### HotPotQA(@yang2018hotpotqa) 🔵

#### Fever(@thorne2018fever) 🔵

#### BBQ: A Hand-Built Bias Benchmark for Question Answering (@parrish2021bbq) 🔵

## Image Prompt Engineering

#### Taksonomia modyfikatorów podpowiedzi (@oppenlaender2022taxonomy)

#### DiffusionDB(@wang2022diffusiondb)

#### The DALLE 2 Prompt Book(@parsons2022dalleprompt) 🔵

#### Prompt Engineering for Text-Based Generative Art(@oppenlaender2022prompt) 🔵

#### Z odpowiednią podpowiedzią, Stable Diffusion 2.0 może zrobić ręce.(@blake2022with) 🔵

#### Optimizing Prompts for Text-to-Image Generation (@hao2022optimizing)

## Prompt Engineering IDEs

#### Prompt IDE(@strobelt2022promptide) 🔵

#### Prompt Source(@bach2022promptsource) 🔵

#### PromptChainer(@wu2022promptchainer) 🔵

#### PromptMaker(@jiang2022promptmaker) 🔵

## Narzędzie

#### LangChain(@Chase_LangChain_2022) 🔵

#### TextBox 2.0: A Text Generation Library with Pre-trained Language Models(@tang2022textbox) 🔵

#### OpenPrompt: An Open-source Framework for Prompt-learning(@ding2021openprompt) 🔵

#### GPT Index(@Liu_GPT_Index_2022) 🔵

# Applied Prompt Engineering

#### Language Model Cascades(@dohan2022language)

#### MRKL(@karpas2022mrkl) 🔵

#### ReAct(@yao2022react) 🔵

#### PAL: Program-aided Language Models(@gao2022pal) 🔵.

## Projektowanie interfejsu użytkownika

#### Design Guidelines for Prompt Engineering Text-to-Image Generative Models (@liu2022design)

# Prompt Injection

#### Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods(@crothers2022machine) 🔵

#### Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples(@branch2022evaluating) 🔵

#### Prompt injection attacks against GPT-3(@simon2022inject) 🔵

#### Wykorzystanie podpowiedzi GPT-3 ze złośliwymi wejściami, które nakazują modelowi zignorować jego poprzednie wskazówki(@goodside2022inject) 🔵

#### adversarial-prompts(@chase2021adversarial) 🔵

#### GPT-3 Prompt Injection Defenses(@goodside2021gpt) 🔵

#### Rozmowy z maszynami: prompt engineering & injection(@christoph2022talking)

#### Exploring Prompt Injection Attacks(@selvi2022exploring) 🔵

#### Używanie GPT-Eliezer przeciwko ChatGPT Jailbreaking(@armstrong2022using) 🔵

#### Microsoft Bing Chat Prompt(@kevinbing)

# Jailbreaking

#### Ignore Previous Prompt: Attack Techniques For Language Models(@perez2022jailbreak)

#### Wnioski dotyczące bezpieczeństwa i niewłaściwego użycia modeli językowych (@brundage_2022)

#### Toxicity Detection with Generative Prompt-based Inference (@wang2022jailbreak)

#### Nowe i ulepszone narzędzia do moderacji treści(@markov_2022)

#### OpenAI API(@openai_api) 🔵

#### OpenAI ChatGPT(@openai_chatgpt) 🔵

#### ChatGPT 4 Tweet(@alice2022jailbreak) 🔵

#### Aktorski Tweet(@miguel2022jailbreak) 🔵

#### Research Tweet(@derek2022jailbreak) 🔵

#### Pretend Ability Tweet(@nero2022jailbreak) 🔵

#### Odpowiedzialność Tweet(@nick2022jailbreak) 🔵

#### Lynx Mode Tweet(@jonas2022jailbreak) 🔵

#### Sudo Mode Tweet(@sudo2022jailbreak) 🔵

#### Ignore Previous Prompt(@ignore_previous_prompt) 🔵

#### Updated Jailbreaking Prompts (@AI_jailbreak) 🔵.

## Surveys

#### Pre-train, Prompt, and Predict: Systematyczny przegląd metod promowania w przetwarzaniu języka naturalnego (@liu2021pretrain)

#### PromptPapers(@ning2022papers)

## Dataset Generation

#### Odkrywanie zachowań modeli językowych za pomocą ocen pisanych przez model (@perez2022discovering)

#### Selective Annotation Makes Language Models Better Few-Shot Learners(@su2022selective)

## Aplikacje

#### Atlas: Few-shot Learning with Retrieval Augmented Language Models(@izacard2022atlas)

#### STRUDEL: Structured Dialogue Summarization for Dialogue Comprehension(@wang2022strudel)

## Miscl

#### Prompting Is Programming: A Query Language For Large Language Models(@beurerkellner2022prompting)

#### Parallel Context Windows Improve In-Context Learning of Large Language Models (@ratner2022parallel)

#### Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models(@bursztyn2022learning)

#### Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks (@wang2022supernaturalinstructions)

#### Making Pre-trained Language Models Better Few-shot Learners (@gao2021making)

#### Uziemienie z wynikami wyszukiwania(@livin2022large)

#### How to Prompt? Możliwości i wyzwania związane z uczeniem się od zera i kilku ujęć dla interakcji człowiek-istota w kreatywnych zastosowaniach modeli generatywnych (@dang2022prompt)

#### O mierzeniu społecznych uprzedzeń w wielozadaniowym uczeniu się opartym na promptach (@akyrek2022measuring)

#### Plot Writing From Pre-Trained Language Models(@jin2022plot) 🔵

#### StereoSet: Measuring stereotypical bias in pretrained language models(@nadeem-etal-2021-stereoset)

#### Badanie halucynacji w generowaniu języka naturalnego(@Ji_2022)

#### Przykłady(@2022examples)

#### Wordcraft(@yuan2022wordcraft)

#### PainPoints(@fadnavis2022pain)

#### Self-Instruct: Aligning Language Model with Self Generated Instructions(@wang2022selfinstruct)

#### From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models(@guo2022images)

#### Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference (@schick2020exploiting)

### Ask-Me-Anything Prompting(@arora2022ama)

### A Watermark for Large Language Models(@kirchenbauer2023watermarking)

